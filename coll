
collect.py

#-*-coding:utf-8-*-

import sys,os,shutil,re,time
import argparse
import ConfigParser 
from lib import *
from sql import *

from lxml.cssselect import CSSSelector
from lxml.etree import fromstring
import lxml.html
import HTMLParser

'''
python collect.py  -i 4 -t 1 -u "http://www.blog.com/index.php/blog/detail/8.html" 

python collect.py  -i 3 -t 1 -u "http://www.blog.com/index.php/blog/index.html?category=1&page=1"

'''


if __name__ == '__main__':
    param = [
      ['-i','--step',int,'this is step'],
      ['-t','--test',int,'this is test'],
      ['-u','--url',str,'this is url']
    ]
    cmd_param = get_cmd_param(param)

    print cmd_param

    step = cmd_param['step']
    test = cmd_param['test']
    _url  = cmd_param['url']

    config = get_config('./conf.ini')
    print config
    db_name = config['db']['name']
    # 连接数据库
    conn = sqlite.connect(os.path.abspath(db_name))
    if step == 1:
        #创建数据库
        print 'step 1 - start'
        create_acticle_db(conn)
        print 'step 1 - ok'
    elif step == 2:
        #初始化 文章地址列表
        print 'step 2 - start'
        with open(os.path.abspath('urls.txt')) as f:
             acticle_urls = f.read()
        acticle_urls_list =  acticle_urls.split('\n')
        print acticle_urls_list
        print len(acticle_urls_list)
        init_acticle_url(conn,acticle_urls_list)
        print 'step 2 - ok'
    elif step == 3:
        if test == 1:
            print _url
            html_content = http_get(_url)
            html_content = html_content.decode('utf-8','ignore')
            html_content = lxml.html.fromstring(html_content)
            tag_a_content = html_content.cssselect(config['url']['a'])
            for tag_a in tag_a_content:
                tag_a_href = tag_a.get("href")
                if tag_a_href.find('http://') > -1:
                    print tag_a_href
                else:
                    print config['global']['domain'] + tag_a_href
                pass
            pass
    elif step == 4:
        if test==1:
            print _url
            html_content = http_get(_url)
            html_content = html_content.decode('utf-8','ignore')
            html_content = lxml.html.fromstring(html_content)
            for field in config['field']:
                print field
                tag_content_list = html_content.cssselect(config['field'][field])
                if len(tag_content_list)>0:
                    tag_content = tag_content_list[0]
                    tag_html_contnet = lxml.html.tostring(tag_content)
                    print HTMLParser.HTMLParser().unescape(tag_html_contnet).encode('gbk','ignore')
                    # print tag_content.text_content().encode('utf-8','ignore')
                pass
            pass
    elif step == 5:
        #开始
        article_urls = get_acticle_url(conn)
        for _url in article_urls:
            print 'urls: '+_url
            html_content = http_get(_url)
            html_content = html_content.decode('utf-8','ignore')
            html_content = lxml.html.fromstring(html_content)
            tag_a_content = html_content.cssselect(config['url']['a'])
            for tag_a in tag_a_content:
                tag_a_href = tag_a.get("href")
                detail_url = ''
                if tag_a_href.find('http://') > -1:
                    detail_url = tag_a_href
                else:
                    detail_url = config['global']['domain'] + tag_a_href
                pass
                time.sleep(1)
                print 'url: '+detail_url
                acticle_html_content = http_get(detail_url)
                acticle_html_content = acticle_html_content.decode('utf-8','ignore')
                acticle_html_content = lxml.html.fromstring(acticle_html_content)
                sql_data = {'url':detail_url} 
                for field in config['field']:
                    tag_content_list = acticle_html_content.cssselect(config['field'][field])
                    if len(tag_content_list)>0:
                        tag_content = tag_content_list[0]
                        tag_html_contnet = lxml.html.tostring(tag_content)
                        field_value = HTMLParser.HTMLParser().unescape(tag_html_contnet)
                        # .encode('utf-8','ignore')
                        # print tag_content.text_content().encode('utf-8','ignore')
                        sql_data[field] = field_value
                    pass
                add_acticle(conn,sql_data)
                time.sleep(2)
                pass
            pass
    pass

conf.ini

[global]

domain = http://www.blog.com

[db]
name = acticle.db

[url]
a = div.content-desc a

[field]

title = div.detail-title > span
content = div.detail-content
other_1 = div.detail-author > span

lib.py


#!/usr/bin/python
#-*-coding:utf-8-*-

import re, sys, time
import httplib, os
from urlparse import urlparse
import argparse
import ConfigParser


#获取 带有前缀的命令行
'''
param = [
    ['-i','--info',str,'this is info'],
    ['-o','--one',int,'this is first'],
    ['-u','--url',str,'this is url']
]
'''
def get_cmd_param(param=None):
    if not param: syntax_error('input param empty !')
    parser = argparse.ArgumentParser()
    # print param
    for x in param:
        parser.add_argument(x[0],x[1],type=x[2],help=x[3])
        pass
    # type = int|str|complax。。。 进行定义
    args = parser.parse_args()
    return vars(args)
    pass

# 请求
def http_get(url,headers=None):
    default_headers = {
            "Accept": "*/*",
            "User-Agent": "Baiduspider+(+http://www.baidu.com/search/spider.htm)",
            "Client-Ip":'180.149.130.112',
            "X-Forwarded-For":'180.149.130.112',
            "Referer": url
        }
    if headers :
        default_headers.update(headers)
    urls = urlparse(url);
    http_type = urls[0]
    domain = urls[1]
    path = urls[2];
    if urls[4]!='' : path += '?' + urls[4]
    # 连接服务器
    dl = httplib.HTTPConnection(domain)
    dl.request(method="GET", url=path, headers=default_headers)
    rs = dl.getresponse()
    if rs.status==200 :
        R = rs.read()
    else :
        print "3 seconds, try again ..."; time.sleep(3)
        dl.request(method="GET", url=path, headers=headers); rs = dl.getresponse()
        if rs.status==200 :
            R = rs.read()
        else :
            print "3 seconds, try again ..."; time.sleep(3)
            dl.request(method="GET", url=path, headers=headers); rs = dl.getresponse()
            if rs.status==200 :
                R = rs.read()
            else :
                print "Continue to collect ..."
                R = 3
    # 返回结果
    return R

# html代码截取函数
def between(html,start,end,cls=''):
    if len(html)==0 : return ;
    # 正则表达式截取
    if start[:1]==chr(40) and start[-1:]==chr(41) and end[:1]==chr(40) and end[-1:]==chr(41) :
        reHTML = re.search(start + '(.*?)' + end,html,re.I)
        if reHTML == None : return 
        reHTML = reHTML.group()
        intStart = re.search(start,reHTML,re.I).end()
        intEnd = re.search(end,reHTML,re.I).start()
        R = reHTML[intStart:intEnd]
    # 字符串截取
    else :
        # 取得开始字符串的位置
        intStart = html.lower().find(start.lower())
        # 如果搜索不到开始字符串，则直接返回空
        if intStart == -1 : return 
        # 取得结束字符串的位置
        intEnd = html[intStart+len(start):].lower().find(end.lower())
        # 如果搜索不到结束字符串，也返回为空
        if intEnd == -1 : return 
        # 开始和结束字符串都有了，可以开始截取了
        R = html[intStart+len(start):intStart+intEnd+len(start)]
    # 清理内容
    if cls != '' :
        R = clear(R,cls)
    # 返回截取的字符
    return R

# 替换实体为正常字符
def en2chr(enStr):
    return enStr.replace('&amp;','&')

# 正则清除
def clear(html,regexs):
    if regexs == '' : return html
    for regex in regexs.split(chr(10)):
        regex = regex.strip()
        if regex != '' :
            if regex[:1]==chr(40) and regex[-1:]==chr(41):
                html = re.sub(regex,'',html,re.I|re.S)
            else :
                html = html.replace(regex,'')
    return html



# 格式化url
def formatURL(html,url):
    urls = re.findall('''(<a[^>]*?href="([^"]+)"[^>]*?>)|(<a[^>]*?href='([^']+)'[^>]*?>)''',html,re.I)
    if urls == None : return html
    for regs in urls :
        html = html.replace(regs[0],matchURL(regs[0],url))
    return html



# 清除html代码里的多余空格
def u_trim(html):
    if len(html) == 0 : return ''
    html = re.sub('\r|\n|\t','',html)
    html = html.replace('  ',' ').strip()
    return html

#正则获取多个
def u_rule(pattern,string,flags=re.I|re.M):
	items = re.findall(pattern,string,flags)
	return items

#正则获取单个
def u_rule_one(pattern,string,flags=re.I|re.M):
	items = u_rule(pattern,string,flags)
	return items[0]


#获取配置
def get_config(file_name):
    out = {}
    cf = ConfigParser.ConfigParser() 
    cf.read(file_name)
    for section in cf.sections():
        items = cf.items(section)
        out[section] = {}
        for item in items:
            out[section][item[0]] = item[1]
            pass
    return out


#时间戳转日期
def time2str(time_stamp=None,format='%Y-%m-%d %H:%M:%S'):
    if time_stamp==None:
        return time.strftime(format,time.localtime(time.time()))
    else:
        return time.strftime(format,time.localtime(time_stamp))

#日期转时间戳
def str2time(time_str,format='%Y-%m-%d %H:%M:%S'):
    if time_str==None: return;
    time.strptime(time_str, format)
    s = time.mktime(time.strptime(time_str, format))
    return int(s)

#调试输出
def dump(var):
    print '#------^_^---------#'
    print var
    print type(var)
    if isinstance(var,list):
        for i in var :
            print i
    elif isinstance(var,dict):
        for i in var:
            print i+":"+var[i]
    print '|------------------|'
    print 


#文件 文件夹是否存在
def is_exist(file_path):
    if os.path.isfile(file_path):
        return True
    else:
        return os.path.exists(file_path)
    pass

#创建目录
def mk_dir(path):
    if not is_exist(path) :
        os.makedirs(path)  

sql.py

#-*-coding:utf-8-*-

try :
    import sqlite3 as sqlite
except ImportError:
    from pysqlite2 import dbapi2 as sqlite

from lib import *

# 创建数据库
def create_acticle_db(conn):
    c = conn.cursor()
    # 文章
    c.execute('''DROP TABLE IF EXISTS "acticle";''')
    c.execute('''
        CREATE TABLE "acticle" (
            "id"  INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
            "url"  TEXT,
            "title"  TEXT,
            "content"  TEXT,
            "other_1"  TEXT,
            "other_2"  TEXT,
            "other_3"  TEXT,
            "other_4"  TEXT,
            "other_5"  TEXT,
            "other_6"  TEXT,
            "other_7"  TEXT,
            "other_8"  TEXT,
            "other_9"  TEXT,
            "other_10"  TEXT,
            "status"  INTEGER,
            "add_time" TEXT,
            "send_time"  TEXT
        );
    ''')
    # 文章链接的列表
    c.execute('''DROP TABLE IF EXISTS "acticle_url";''')
    c.execute('''
        CREATE TABLE "acticle_url" (
            "id"  INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
            "url"  TEXT,
            "status"  INTEGER,
            "add_time" TEXT
        );
    ''')
    #提交
    conn.commit()
    c.close()

#执行sql
def execute(conn,sql,data=[]):
	if sql is not None and sql != '':
		c = conn.cursor()
		c.execute(sql,data)
		conn.commit()
		c.close()

#查询
def query(conn,sql=None,data=None):
	if not data:data = []
	out = None
	if sql and sql != '':
		c = conn.cursor()
		c.execute(sql,data)
		record = c.fetchall()
		field = [i[0] for i in c.description]
		out = [dict(zip(field,i)) for i in record]
	return out

#初始化 acticle_url
def init_acticle_url(conn,urls=None):
	c = conn.cursor()
	for url in urls:
		if not url : continue
		sql = 'INSERT INTO acticle_url (url,status,add_time)values (?, ?, ?)'
		add_time = time2str()
		data = [url, 1,add_time]
		c.execute(sql,data)
		conn.commit()
		pass
	c.close()

#文章列表数据
def get_acticle_url(conn):
	sql = "SELECT * FROM acticle_url"
	res = query(conn,sql)
	out = []
	for x in res:
		out.append(x['url'])
		pass
	return out

#添加
def add_acticle(conn,data):
	fields = []
	fields_data = []
	fields_data_pos = [] 
	for field in data:
		fields.append(field)
		fields_data.append(data[field])
		fields_data_pos.append('?')
		pass
	sql = 'INSERT INTO acticle ('+','.join(fields)+') values ('+','.join(fields_data_pos)+') '
	execute(conn,sql,fields_data)
	pass
  
urls.txt
  
http://www.blog.com/index.php/blog/index.html?category=1&page=1
http://www.blog.com/index.php/blog/index.html?category=1&page=2


